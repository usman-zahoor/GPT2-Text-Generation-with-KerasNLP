{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anurag20072002/Data-Dreamers-5201/blob/main/Copy_of_gpt2_text_generation_with_kerasnlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz7eQ1ci146f"
      },
      "source": [
        "# GPT2 Text Generation with KerasNLP\n",
        "\n",
        "**Author:** Chen Qian<br>\n",
        "**Date created:** 2023/04/17<br>\n",
        "**Last modified:** 2024/04/12<br>\n",
        "**Description:** Use KerasNLP GPT2 model and `samplers` to do text generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUttkuyX146l"
      },
      "source": [
        "In this tutorial, you will learn to use [KerasNLP](https://keras.io/keras_nlp/) to load a\n",
        "pre-trained Large Language Model (LLM) - [GPT-2 model](https://openai.com/research/better-language-models)\n",
        "(originally invented by OpenAI), finetune it to a specific text style, and\n",
        "generate text based on users' input (also known as prompt). You will also learn\n",
        "how GPT2 adapts quickly to non-English languages, such as Chinese."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5H8AniA146m"
      },
      "source": [
        "##  Before we begin\n",
        "\n",
        "Colab offers different kinds of runtimes. Make sure to go to **Runtime ->\n",
        "Change runtime type** and choose the GPU Hardware Accelerator runtime\n",
        "(which should have >12G host RAM and ~15G GPU RAM) since you will finetune the\n",
        "GPT-2 model. Running this tutorial on CPU runtime will take hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NidEW_2B146n"
      },
      "source": [
        "## Install KerasNLP, Choose Backend and Import Dependencies\n",
        "\n",
        "This examples uses [Keras 3](https://keras.io/keras_3/) to work in any of\n",
        "`\"tensorflow\"`, `\"jax\"` or `\"torch\"`. Support for Keras 3 is baked into\n",
        "KerasNLP, simply change the `\"KERAS_BACKEND\"` environment variable to select\n",
        "the backend of your choice. We select the JAX backend below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dS_tgdje146o",
        "outputId": "be38ee59-c28b-40d4-d347-25c08c0ce50b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/keras-team/keras-nlp.git -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyK3XzVW146r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
        "\n",
        "import keras_nlp\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOWch1I2146s"
      },
      "source": [
        "## Introduction to Generative Large Language Models (LLMs)\n",
        "\n",
        "Large language models (LLMs) are a type of machine learning models that are\n",
        "trained on a large corpus of text data to generate outputs for various natural\n",
        "language processing (NLP) tasks, such as text generation, question answering,\n",
        "and machine translation.\n",
        "\n",
        "Generative LLMs are typically based on deep learning neural networks, such as\n",
        "the [Transformer architecture](https://arxiv.org/abs/1706.03762) invented by\n",
        "Google researchers in 2017, and are trained on massive amounts of text data,\n",
        "often involving billions of words. These models, such as Google [LaMDA](https://blog.google/technology/ai/lamda/)\n",
        "and [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html),\n",
        "are trained with a large dataset from various data sources which allows them to\n",
        "generate output for many tasks. The core of Generative LLMs is predicting the\n",
        "next word in a sentence, often referred as **Causal LM Pretraining**. In this\n",
        "way LLMs can generate coherent text based on user prompts. For a more\n",
        "pedagogical discussion on language models, you can refer to the\n",
        "[Stanford CS324 LLM class](https://stanford-cs324.github.io/winter2022/lectures/introduction/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7fLM8gX146t"
      },
      "source": [
        "## Introduction to KerasNLP\n",
        "\n",
        "Large Language Models are complex to build and expensive to train from scratch.\n",
        "Luckily there are pretrained LLMs available for use right away. [KerasNLP](https://keras.io/keras_nlp/)\n",
        "provides a large number of pre-trained checkpoints that allow you to experiment\n",
        "with SOTA models without needing to train them yourself.\n",
        "\n",
        "KerasNLP is a natural language processing library that supports users through\n",
        "their entire development cycle. KerasNLP offers both pretrained models and\n",
        "modularized building blocks, so developers could easily reuse pretrained models\n",
        "or stack their own LLM.\n",
        "\n",
        "In a nutshell, for generative LLM, KerasNLP offers:\n",
        "\n",
        "- Pretrained models with `generate()` method, e.g.,\n",
        "    `keras_nlp.models.GPT2CausalLM` and `keras_nlp.models.OPTCausalLM`.\n",
        "- Sampler class that implements generation algorithms such as Top-K, Beam and\n",
        "    contrastive search. These samplers can be used to generate text with\n",
        "    custom models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lJOu6sp146t"
      },
      "source": [
        "## Load a pre-trained GPT-2 model and generate some text\n",
        "\n",
        "KerasNLP provides a number of pre-trained models, such as [Google\n",
        "Bert](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)\n",
        "and [GPT-2](https://openai.com/research/better-language-models). You can see\n",
        "the list of models available in the [KerasNLP repository](https://github.com/keras-team/keras-nlp/tree/master/keras_nlp/models).\n",
        "\n",
        "It's very easy to load the GPT-2 model as you can see below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecNa6wlG146u",
        "outputId": "76798759-8ab9-4905-a197-712c23627f97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/model.safetensors...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/model.safetensors.index.json...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/metadata.json...\n",
            "100%|██████████| 141/141 [00:00<00:00, 124kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/preprocessor.json...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/tokenizer.json...\n",
            "100%|██████████| 448/448 [00:00<00:00, 494kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/assets/tokenizer/vocabulary.json...\n",
            "100%|██████████| 0.99M/0.99M [00:00<00:00, 1.80MB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/assets/tokenizer/merges.txt...\n",
            "100%|██████████| 446k/446k [00:00<00:00, 999kB/s] \n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/model.safetensors...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/model.safetensors.index.json...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/task.json...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/config.json...\n",
            "100%|██████████| 484/484 [00:00<00:00, 449kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/model.safetensors...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/model.safetensors.index.json...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/model.weights.h5...\n",
            "100%|██████████| 475M/475M [00:14<00:00, 33.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# To speed up training and generation, we use preprocessor of length 128\n",
        "# instead of full length 1024.\n",
        "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=128,\n",
        ")\n",
        "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "    \"gpt2_base_en\", preprocessor=preprocessor\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S3DMGiy146v"
      },
      "source": [
        "Once the model is loaded, you can use it to generate some text right away. Run\n",
        "the cells below to give it a try. It's as simple as calling a single function\n",
        "*generate()*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cGXAg0f146w",
        "outputId": "588f66c9-33c7-4e32-8e65-806162560dce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "My trip to Yosemite was one of those things where it's easy to see why the world is full of amazing places, but it's hard to see the whole picture when you have the whole world in front of you. I spent a great deal of time in Yosemite and I've never had to walk through the park in the whole day, and there were some amazing things I could not get a good look at. I've also had a lot of fun with the Yosemite National Park. It was one of those things where it's easy to see why the world is full of amazing places, but it's hard to see the whole picture when you have the whole world in front of you.\n",
            "\n",
            "I've had a lot of fun with the Yosemite National Park. It was one of those things where it's easy to see why the world is full of amazing places, but it's hard to see the whole picture when you have the whole world in front of you.\n",
            "\n",
            "I love the\n",
            "TOTAL TIME ELAPSED: 10.02s\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "output = gpt2_lm.generate(\"My trip to Yosemite was\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKH43pIY146w"
      },
      "source": [
        "Try another one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yijGJKEn146x",
        "outputId": "15905204-baa4-4d0b-ed57-0b9074f96b1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "That Italian restaurant is the best in town and the best in town for a good Italian meal! It's a great place to eat Italian food. We have Italian dishes on the menu, but we have no complaints here.\n",
            "\n",
            "The menu is a lot of what you expect from Italian restaurants, but there are some really good things about this one. It has a great Italian restaurant with great food, and a good atmosphere. The food is very good. It's a nice place to go for lunch and dinner. The service is very friendly. The food is good and the atmosphere is good. The staff is nice, and the food is good. The service is very good and the atmosphere is good. The restaurant is a little crowded, but the service is good, the food is excellent, and the atmosphere is good.\n",
            "\n",
            "This place is the perfect place for a quick dinner. The food is good and the atmosphere is nice. The place has an Italian vibe and I love the place\n",
            "TOTAL TIME ELAPSED: 2.01s\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "output = gpt2_lm.generate(\"That Italian restaurant is\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flfPh59R146x"
      },
      "source": [
        "Notice how much faster the second call is. This is because the computational\n",
        "graph is [XLA compiled](https://www.tensorflow.org/xla) in the 1st run and\n",
        "re-used in the 2nd behind the scenes.\n",
        "\n",
        "The quality of the generated text looks OK, but we can improve it via\n",
        "fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXvCkLEo146x"
      },
      "source": [
        "## More on the GPT-2 model from KerasNLP\n",
        "\n",
        "Next up, we will actually fine-tune the model to update its parameters, but\n",
        "before we do, let's take a look at the full set of tools we have to for working\n",
        "with for GPT2.\n",
        "\n",
        "The code of GPT2 can be found\n",
        "[here](https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/models/gpt2/).\n",
        "Conceptually the `GPT2CausalLM` can be hierarchically broken down into several\n",
        "modules in KerasNLP, all of which have a *from_preset()* function that loads a\n",
        "pretrained model:\n",
        "\n",
        "- `keras_nlp.models.GPT2Tokenizer`: The tokenizer used by GPT2 model, which is a\n",
        "    [byte-pair encoder](https://huggingface.co/course/chapter6/5?fw=pt).\n",
        "- `keras_nlp.models.GPT2CausalLMPreprocessor`: the preprocessor used by GPT2\n",
        "    causal LM training. It does the tokenization along with other preprocessing\n",
        "    works such as creating the label and appending the end token.\n",
        "- `keras_nlp.models.GPT2Backbone`: the GPT2 model, which is a stack of\n",
        "    `keras_nlp.layers.TransformerDecoder`. This is usually just referred as\n",
        "    `GPT2`.\n",
        "- `keras_nlp.models.GPT2CausalLM`: wraps `GPT2Backbone`, it multiplies the\n",
        "    output of `GPT2Backbone` by embedding matrix to generate logits over\n",
        "    vocab tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_z-KQrU146y"
      },
      "source": [
        "## Finetune on imbd reviews dataset\n",
        "\n",
        "Now you have the knowledge of the GPT-2 model from KerasNLP, you can take one\n",
        "step further to finetune the model so that it generates text in a specific\n",
        "style, short or long, strict or casual. In this tutorial, we will use imbd\n",
        "dataset for example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWynXb5y146y"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "imdb_reviews_ds = tfds.load(\"imdb_reviews\", split=\"train\", as_supervised=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0PNWJF1146y"
      },
      "source": [
        "Let's take a look inside sample data from the reddit TensorFlow Dataset. There\n",
        "are two features:\n",
        "\n",
        "- **__document__**: text of the post.\n",
        "- **__title__**: the title."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wSzATrAQO8NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IedX7vO0146y",
        "outputId": "a7122bd8-81e8-4d5c-9a78-77b8a6b91231",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "for document, title in imdb_reviews_ds:\n",
        "    print(document.numpy())\n",
        "    print(title.numpy())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZawqT-9s146z"
      },
      "source": [
        "In our case, we are performing next word prediction in a language model, so we\n",
        "only need the 'document' feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vvvua7WQ146z"
      },
      "outputs": [],
      "source": [
        "train_ds = (\n",
        "    imdb_reviews_ds.map(lambda document, _: document)\n",
        "    .batch(32)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9LoIj3a146z"
      },
      "source": [
        "Now you can finetune the model using the familiar *fit()* function. Note that\n",
        "`preprocessor` will be automatically called inside `fit` method since\n",
        "`GPT2CausalLM` is a `keras_nlp.models.Task` instance.\n",
        "\n",
        "This step takes quite a bit of GPU memory and a long time if we were to train\n",
        "it all the way to a fully trained state. Here we just use part of the dataset\n",
        "for demo purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ2dLcp61460",
        "outputId": "b576b336-bdae-43d0-cbcf-da9119760fbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 335ms/step - accuracy: 0.3225 - loss: 3.6373\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d4c25bef250>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train_ds = train_ds.take(500)\n",
        "num_epochs = 1\n",
        "\n",
        "# Linearly decaying learning rate.\n",
        "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
        "    5e-5,\n",
        "    decay_steps=train_ds.cardinality() * num_epochs,\n",
        "    end_learning_rate=0.0,\n",
        ")\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "gpt2_lm.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "gpt2_lm.fit(train_ds, epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5elg8hzr1460"
      },
      "source": [
        "After fine-tuning is finished, you can again generate text using the same\n",
        "*generate()* function. This time, the text will be closer to Reddit writing\n",
        "style, and the generated length will be close to our preset length in the\n",
        "training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COAVm1xz1460",
        "outputId": "842dba5a-7b3e-4728-afb7-752ccdcca202",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "I like basketball, but I don't like watching it. The movie is very bad and I don't like the characters. I like the acting, but it doesn't make sense. The acting sucks, but there are a few good ones. The only one that really made sense was when the two girls get together and start to kiss each other. I think they are going to get married soon, but it seems like it's not going to happen. I think that it is going to be a very bad movie.\n",
            "TOTAL TIME ELAPSED: 7.23s\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z50hflUY1460"
      },
      "source": [
        "## Into the Sampling Method\n",
        "\n",
        "In KerasNLP, we offer a few sampling methods, e.g., contrastive search,\n",
        "Top-K and beam sampling. By default, our `GPT2CausalLM` uses Top-k search, but\n",
        "you can choose your own sampling method.\n",
        "\n",
        "Much like optimizer and activations, there are two ways to specify your custom\n",
        "sampler:\n",
        "\n",
        "- Use a string identifier, such as \"greedy\", you are using the default\n",
        "configuration via this way.\n",
        "- Pass a `keras_nlp.samplers.Sampler` instance, you can use custom configuration\n",
        "via this way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwyfLFi41461",
        "outputId": "fb3793a9-d341-4786-9a3a-0b18638c2c5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "I like basketball, but I don't like the way it's done. I'm not a big fan of the NBA, but I'm a fan of basketball. I think this film is a great way to get some laughs out of this movie. The film was filmed on a rainy night with no sunlight and I had to sit and wait for the rain to fall. I think I was in a good mood, I just couldn't sit still. I was in awe of the film, I was amazed how it was made and I was so glad it was made. I think it has the potential of making a great movie, I hope to see\n",
            "\n",
            "GPT-2 output:\n",
            "I like basketball, but I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the movie. I don't like the\n"
          ]
        }
      ],
      "source": [
        "# Use a string identifier.\n",
        "gpt2_lm.compile(sampler=\"top_k\")\n",
        "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)\n",
        "\n",
        "# Use a `Sampler` instance. `GreedySampler` tends to repeat itself,\n",
        "greedy_sampler = keras_nlp.samplers.GreedySampler()\n",
        "gpt2_lm.compile(sampler=greedy_sampler)\n",
        "\n",
        "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymM5sRvL1461"
      },
      "source": [
        "For more details on KerasNLP `Sampler` class, you can check the code\n",
        "[here](https://github.com/keras-team/keras-nlp/tree/master/keras_nlp/samplers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlkXYinn1461"
      },
      "source": [
        "## Finetune on Chinese Poem Dataset\n",
        "\n",
        "We can also finetune GPT2 on non-English datasets. For readers knowing Chinese,\n",
        "this part illustrates how to fine-tune GPT2 on Chinese poem dataset to teach our\n",
        "model to become a poet!\n",
        "\n",
        "Because GPT2 uses byte-pair encoder, and the original pretraining dataset\n",
        "contains some Chinese characters, we can use the original vocab to finetune on\n",
        "Chinese dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XGQ0RWf1461",
        "outputId": "e8811d3a-2fbe-49a9-c060-c91e677cd61f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chinese-poetry'...\n",
            "remote: Enumerating objects: 7323, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 7323 (delta 5), reused 10 (delta 3), pack-reused 7309\u001b[K\n",
            "Receiving objects: 100% (7323/7323), 236.98 MiB | 22.95 MiB/s, done.\n",
            "Resolving deltas: 100% (5003/5003), done.\n",
            "Updating files: 100% (2285/2285), done.\n"
          ]
        }
      ],
      "source": [
        "!# Load chinese poetry dataset.\n",
        "!git clone https://github.com/chinese-poetry/chinese-poetry.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdy2kX2M1462"
      },
      "source": [
        "Load text from the json file. We only use《全唐诗》for demo purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LklCerB1462"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "poem_collection = []\n",
        "for file in os.listdir(\"chinese-poetry/全唐诗\"):\n",
        "    if \".json\" not in file or \"poet\" not in file:\n",
        "        continue\n",
        "    full_filename = \"%s/%s\" % (\"chinese-poetry/全唐诗\", file)\n",
        "    with open(full_filename, \"r\") as f:\n",
        "        content = json.load(f)\n",
        "        poem_collection.extend(content)\n",
        "\n",
        "paragraphs = [\"\".join(data[\"paragraphs\"]) for data in poem_collection]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mKw6Nm01463"
      },
      "source": [
        "Let's take a look at sample data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXsLv0XS1463",
        "outputId": "78107f63-a028-462e-8d4b-fe9451179732",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "短檠三尺照座隅，眵昏兩目頭不梳。丈夫功業務廣大，安用事此牛尾書。多求舊聞助器識，欲駕萬里須舟輿。要堅志節在專苦，積螢照夜真前車。道鄉先生好門戶，髯季晚出充門閭。昻昻鷄群見野鶴，炯炯虎視嗟黔驢。如何天公不着眼，棄此異寶猶紛挐。我知造物自有意，將騁健駿先虛徐。金須百錬作鐘鼎，玉試三火真璠璵。來年明光再射策，聊取髙第酬三餘。古今人事自差别，見晚用速皆乘除。他年雲路着鞭穩，無忘過我中田廬。\n"
          ]
        }
      ],
      "source": [
        "print(paragraphs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0agWH921463"
      },
      "source": [
        " we convert to TF dataset, and only use partial data\n",
        "to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI3USD-v1464",
        "outputId": "f00ac6b4-0924-4a9f-cc65-17143d21448c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 233ms/step - accuracy: 0.2356 - loss: 2.9264\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d4c26277580>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(paragraphs)\n",
        "    .batch(16)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Running through the whole dataset takes long, only take `500` and run 1\n",
        "# epochs for demo purposes.\n",
        "train_ds = train_ds.take(500)\n",
        "num_epochs = 1\n",
        "\n",
        "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
        "    5e-4,\n",
        "    decay_steps=train_ds.cardinality() * num_epochs,\n",
        "    end_learning_rate=0.0,\n",
        ")\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "gpt2_lm.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "gpt2_lm.fit(train_ds, epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZLF5HT31464"
      },
      "source": [
        "Let's check the result!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydZft7Vm1465",
        "outputId": "b8f513b8-37f8-4a4a-d931-f101dafa51fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "昨夜雨疏风骤馬，萬渾書風萬秋。書頭秋秋萬風，居翠江倚汝頻。秋秋曲萬霜著，江風書爭欲樂。\n"
          ]
        }
      ],
      "source": [
        "output = gpt2_lm.generate(\"昨夜雨疏风骤\", max_length=200)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#titanic dataset"
      ],
      "metadata": {
        "id": "3Xj7cOz-9INi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json # Import the json module\n",
        "\n",
        "with open('/content/titanic.json', 'r', encoding='utf-8') as f:\n",
        "    titanic_data = json.load(f) # Now you can use json.load()"
      ],
      "metadata": {
        "id": "WPgypxdR2gtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TensorFlow dataset from the JSON data\n",
        "import tensorflow as tf\n",
        "texts = titanic_data[\"text\"]\n",
        "titanic_ds = tf.data.Dataset.from_tensor_slices(texts)"
      ],
      "metadata": {
        "id": "sLCHxpGp2gqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset for training\n",
        "train_ds = (\n",
        "    titanic_ds\n",
        "    .batch(32)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "PVFoPt8o2gnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a subset for training (optional, depending on your needs)\n",
        "train_ds = train_ds.take(1000)\n",
        "num_epochs = 1\n",
        "\n",
        "# Configure a dynamically decreasing learning rate based on linear decay\n",
        "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=5e-5,\n",
        "    decay_steps=train_ds.cardinality() * num_epochs,\n",
        "    end_learning_rate=0.0,\n",
        ")"
      ],
      "metadata": {
        "id": "qs5vCY_K2glU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Assuming gpt2_lm is your GPT-2 model\n",
        "# If not already defined, you need to load or create your GPT-2 model here\n",
        "\n",
        "# Model compilation\n",
        "gpt2_lm.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")\n",
        "# Train the model\n",
        "gpt2_lm.fit(train_ds, epochs=num_epochs)"
      ],
      "metadata": {
        "id": "cfVgxKIA2giy",
        "outputId": "7952560e-ee33-4468-c755-978372ab2250",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 18s/step - accuracy: 0.1468 - loss: 0.8197\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d4c0afc78e0>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "output = gpt2_lm.generate(\"I'm the king\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)\n",
        "end = time.time()\n",
        "print(f\"TOTAL TIME ELAPSED: {end-start:.2f}s\")"
      ],
      "metadata": {
        "id": "63wMVh6w2ggL",
        "outputId": "475f0b94-fa9e-447d-b74a-5e83c5c54a6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "I'm the king of the world, I don\n",
            "TOTAL TIME ELAPSED: 0.18s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real time Dataset"
      ],
      "metadata": {
        "id": "v-wUV65N3Ggc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('/content/realtime_dataset.json', 'r', encoding='utf-8') as f:\n",
        "    realtime_data=json.load(f)"
      ],
      "metadata": {
        "id": "E6LrK49s2gds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract texts from the dataset\n",
        "texts = [realtime_data.get('text', '')]  # Access the 'text' value directly, or provide an empty string if it doesn't exist\n",
        "\n",
        "# Create a TensorFlow dataset from the texts\n",
        "realtime_ds = tf.data.Dataset.from_tensor_slices(texts)\n",
        "\n",
        "# Prepare the dataset for training\n",
        "train_ds = (\n",
        "    realtime_ds\n",
        "    .batch(32)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "QSNKdySY2gbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a subset for training (optional, depending on your needs)\n",
        "train_ds = train_ds.take(1000)\n",
        "num_epochs = 3\n",
        "\n",
        "# Configure a dynamically decreasing learning rate based on linear decay\n",
        "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=5e-5,\n",
        "    decay_steps=train_ds.cardinality() * num_epochs,\n",
        "    end_learning_rate=0.0,\n",
        ")\n",
        "\n",
        "# Define the loss function\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Model compilation\n",
        "gpt2_lm.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "gpt2_lm.fit(train_ds, epochs=num_epochs)"
      ],
      "metadata": {
        "id": "L07sIy9T3TKj",
        "outputId": "63adfad4-971f-4f1f-98e2-b4187688055e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21s/step - accuracy: 0.2031 - loss: 4.5116\n",
            "Epoch 2/3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 16s/step - accuracy: 0.2578 - loss: 4.6216\n",
            "Epoch 3/3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.2188 - loss: 4.6375\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d4c0c2318d0>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "start = time.time()\n",
        "output = gpt2_lm.generate(\"the god is\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)\n",
        "end = time.time()\n",
        "print(f\"TOTAL TIME ELAPSED: {end-start:.2f}s\")"
      ],
      "metadata": {
        "id": "VL6HzF023TE3",
        "outputId": "4cc78f65-f6ae-4d55-f594-f6adc6660936",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "the god is it the best of the old, the most important of the world, and I can't say it doesn\n",
            "TOTAL TIME ELAPSED: 0.33s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}